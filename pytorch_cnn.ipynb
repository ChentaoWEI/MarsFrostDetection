{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import json\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import logging\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import PIL.Image as Image\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (299, 299)  # All images contained in this dataset are 299x299 (originally, to match Inception v3 input size)\n",
    "SEED = 17\n",
    "\n",
    "# Head directory containing all image subframes. Update with the relative path of your data directory\n",
    "data_head_dir = Path('../data')\n",
    "\n",
    "# Find all subframe directories\n",
    "subdirs = [Path(subdir.stem) for subdir in data_head_dir.iterdir() if subdir.is_dir()]\n",
    "src_image_ids = ['_'.join(a_path.name.split('_')[:3]) for a_path in subdirs]\n",
    "\n",
    "# Load train/val/test subframe IDs\n",
    "def load_text_ids(file_path):\n",
    "    \"\"\"Simple helper to load all lines from a text file\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = [line.strip() for line in f.readlines()]\n",
    "    return lines\n",
    "\n",
    "# Load the subframe names for the three data subsets\n",
    "train_ids = load_text_ids('./train_source_images.txt')\n",
    "validate_ids = load_text_ids('./val_source_images.txt')\n",
    "test_ids = load_text_ids('./test_source_images.txt')\n",
    "\n",
    "# Generate a list containing the dataset split for the matching subdirectory names\n",
    "subdir_splits = []\n",
    "for src_id in src_image_ids:\n",
    "    if src_id in train_ids:\n",
    "        subdir_splits.append('train')\n",
    "    elif src_id in validate_ids:\n",
    "        subdir_splits.append('validate')\n",
    "    elif(src_id in test_ids):\n",
    "        subdir_splits.append('test')\n",
    "    else:\n",
    "        logging.warning(f'{src_id}: Did not find designated split in train/validate/test list.')\n",
    "        subdir_splits.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_subdir_data(dir_path, image_size, seed=None): \n",
    "    # Grab only the classes that (1) we want to keep and (2) exist in this directory\n",
    "    tile_dir = dir_path / Path('tiles')\n",
    "    label_dir = dir_path / Path('labels')\n",
    "    \n",
    "    loc_list = []\n",
    "    \n",
    "    for folder in os.listdir(tile_dir):\n",
    "        if os.path.isdir(os.path.join(tile_dir, folder)):\n",
    "            for file in os.listdir(os.path.join(tile_dir, folder)):\n",
    "                if file.endswith(\".png\"):\n",
    "                    loc_list.append((os.path.join(os.path.join(tile_dir, folder), file), folder))\n",
    "\n",
    "    return loc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test, data_val = [], [], []\n",
    "for subdir, split in zip(subdirs, subdir_splits):\n",
    "    full_path = data_head_dir / subdir\n",
    "    if split=='validate':\n",
    "        data_val.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "    elif split=='train':\n",
    "        data_train.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "    elif split=='test':\n",
    "        data_test.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
    "random.shuffle(data_train)\n",
    "random.shuffle(data_test)\n",
    "random.shuffle(data_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_list, label_list, transform=None):\n",
    "        self.img_list = img_list\n",
    "        self.label_list = label_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.img_list[idx]).convert('RGB')\n",
    "        label = 1 if self.label_list[idx] == 'frost' else 0\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "img_list_train, label_list_train = zip(*data_train)\n",
    "img_list_test, label_list_test = zip(*data_test)\n",
    "img_list_val, label_list_val = zip(*data_val)\n",
    "train_dataset = CustomDataset(img_list_train, label_list_train, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_dataset = CustomDataset(img_list_test, label_list_test, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)\n",
    "val_dataset = CustomDataset(img_list_val, label_list_val, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.fc = nn.Linear(128 * 299 * 299, 256)  # Assuming image size is 299x299\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(F.relu(self.fc(x)))\n",
    "        return x\n",
    "\n",
    "model = CustomModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-5)  # L2 regularization\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the lists to store the training and validation loss\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 5\n",
    "trigger_times = 0\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# Run the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader: \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    # Append the training and validation loss to the lists\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "# Plot the training and validation loss\n",
    "plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses)+1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
